
# Core dependencies
torch>=2.0.0
transformers>=4.30.0
pandas>=1.3.0
numpy>=1.20.0
scikit-learn>=1.0.0
tqdm>=4.62.0

# For efficient CPU training
bitsandbytes>=0.40.0
peft>=0.4.0
accelerate>=0.20.0

# Optional dependencies
matplotlib>=3.5.0
seaborn>=0.11.0
ipywidgets>=7.7.0

# For logging and experiment tracking
tensorboard>=2.10.0




############# Phi 
torch==2.1.2+cu118
torchvision==0.16.2+cu118
torchaudio==2.1.2+cu118
--extra-index-url https://download.pytorch.org/whl/cu118

transformers>=4.36.0
accelerate>=0.26.0
bitsandbytes==0.41.3
peft>=0.6.0

pandas
numpy<2
scikit-learn
tqdm


############ Gemma
# Core libraries for fine-tuning LLMs
transformers==4.39.3
datasets==2.18.0
accelerate==0.29.3
peft==0.10.0  # Optional: For efficient fine-tuning like LoRA

# Tokenization and model utilities
sentencepiece==0.1.99  # Required by some tokenizer models (e.g., LLaMA, Gemma)
scipy>=1.7.3
pandas>=1.5.0

# PyTorch - specify your version based on CUDA
# Use the correct version for your clusterâ€™s CUDA setup
torch==2.2.2  # For CUDA 11.8 (adjust if using CUDA 12.x)
# If youâ€™re using pip wheels from pytorch.org, use:
# pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118

# Optional utilities
tqdm



################### Granite
torch>=2.0.0
transformers>=4.30.0
datasets>=2.12.0
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.2.0
tqdm>=4.65.0
wandb>=0.15.0
accelerate>=0.20.0
bitsandbytes>=0.41.0
sentencepiece>=0.1.99
peft>=0.4.0
tensorboard>=2.13.0


############### transformers
torch>=2.0.0
pandas
numpy
scikit-learn
